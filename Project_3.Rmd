---
title: "Project 3"
author: "Suyog Dharmadhikari; Bennett McAuley"
date: "2022-11-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(152)

library(tidyverse)
library(readr)
library(caret)
library(ggplot2)
library(corrplot)
```

## Introduction

The goal of this project is to create predictive models and automating the process of generating Markdown reports. The analyses will make use of the [Online News Popularity](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) data from the UCI Machine Learning Repository.

This data contains information about articles published by Mashable over a period of two years--specifically the statistics associated with them.

The purpose of this analysis is to use linear regression and ensemble tree-based methods to predict the number of `shares`--our target variable--subset by data channel. For more details about these methods, see the _Modeling_ section.

According to the data description, there are 39,797 observations and 61 variables present. 58 of them are predictors, but using all of them would be inefficient and show glaring redundancies. Instead, the ones that will be used were chosen intuitively--what might encourage a user to share an article and what underlying forces may influence them:

1. `average_token_length` - Average length of the words in the content
2. `avg_negative_polarity` - Avg. polarity of negative words
3. `global_rate_positive_words` - Rate of positive words in the content
4. `global_sentiment_polarity` - Text sentiment polarity
5. `global_subjectivity` - Text subjectivity
6. `is_weekend` - Was the article published on the weekend?
7. `kw_avg_max` - Best keyword (avg. shares)
8. `kw_avg_min` - Worst Keyword (avg. shares)
9. `kw_max_max` - Best keyword (max shares)
10. `kw_min_min` - Worst keyword (min shares)
11. `n_non_stop_words` - Rate of non-stop words in the content
12. `n_tokens_content` - Number of words in the content
13. `n_tokens_title` - Number of words in the title
14. `n_unique_tokens` - Rate of unique words in the content
15. `num_hrefs` - Number of links
16. `num_imgs` - Number of images
17. `num_self_hrefs` - Number of links to other articles published by Mashable
18. `num_videos` - Number of videos
19. `self_reference_avg_shares` - Avg. shares of referenced articles in Mashable
20. `title_sentiment_polarity` - Title polarity

Which we store in a vector for fast retrieval:
```{r vars}
news.vars <- c('n_tokens_title', 'n_tokens_content', 'n_non_stop_words', 'num_hrefs', 'num_self_hrefs',
               'num_imgs', 'num_videos', 'average_token_length', 'kw_max_max', 'kw_min_min',
               'kw_avg_max', 'kw_avg_min', 'self_reference_avg_sharess','is_weekend', 'global_subjectivity',
               'global_sentiment_polarity', 'global_rate_positive_words', 'avg_negative_polarity', 'n_unique_tokens', 'title_sentiment_polarity')
```

## The Data

For the first iteration of our analysis, we will be using a single source for data channel. In other words, we will subset the data to one data channel here, and later, automate the process across all of the channels.

So, we read in the data and subset it to our chosen variables (`is_weekend` converted into factor) and the `Tech` data channel:

```{r read data}
news <- read_csv("OnlineNewsPopularity.csv")

tech.news <- news %>% filter(data_channel_is_tech == 1) %>% 
  select(all_of(news.vars), shares) %>%
  mutate(is_weekend = as.factor(is_weekend))

head(tech.news)
```
## Summarizations
Now that our data has been manipulated and filtered, we can perform a simple EDA.

### Summary Statistics
First and foremost, we want to know what range of values we might expect when predicting for `shares`. The following table shows basic summary statistics about the variable:

```{r five_sum}
s <- summary(tech.news$shares)

tibble(var = 'shares', min = s[1], max = s[6], mean = s[3], stddev = sd(tech.news$shares))
```

```{r sumData, echo=TRUE, eval=TRUE,warning=FALSE,message=FALSE}

  countNonStopWords <- tech.news %>% 
  group_by(global_subjectivity) %>%
  summarise(total_count=n(),
            .groups = 'drop')
  
  countNonStopWords
  
```
If the maximum is tremendously larger in scale than the mean, then it is very likely an outlier. If the standard deviation is larger than the mean, then the number of shares varies greatly. Furthermore, the higher it is, the more disperse they are from the mean. The inverse also applies for a smaller standard deviation (i.e. the smaller, the less disperse).

### Plots
For visual analysis, we first construct a density plot to compare the distribution of `shares` between the weekend (`is_weekend = 1`) and weekdays (`is_weekend = 0`). Observe the patterns generated for similarities and/or differences.

```{r density}
g <- ggplot(data = tech.news, aes(x = shares))

g + geom_density(adjust = 0.5, alpha = 0.5, aes(fill = is_weekend)) +
  xlim(0, 8000)
```

The second visual is a scatterplot with a trend line. The number of words in the content is on the x-axis and the rate of unique words on the y-axis. If the data points and trend line show an upward trend, then articles with more words tend to have a larger rate of unique ones. If there is a downward trend, then articles with more words tend to have a smaller rate of unique ones.

```{r scatter}
p <- ggplot(tech.news, aes(x = n_tokens_content, y = n_unique_tokens))

p + geom_point() +
  geom_smooth(method = glm, col = "Blue")
```

## Modeling
The goal now is to create models for predicting the number of `shares` using linear regression and ensemble trees. Before that, the data is split into a training (70%) and test (30%) set.

```{r split}
i <- createDataPartition(tech.news[[1]], p = 0.75, list = FALSE)

train <- tech.news[i,]
test <- tech.news[-i,]
```