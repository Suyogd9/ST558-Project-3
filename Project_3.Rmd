---
title: "Project 3"
author: "Suyog Dharmadhikari; Bennett McAuley"
date: "2022-11-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(152)

library(tidyverse)
library(readr)
```

## Introduction

The goal of this project is to create predictive models and automating the process of generating Markdown reports. The analyses will make use of the [Online News Popularity](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) data from the UCI Machine Learning Repository.

This data contains information about articles published by Mashable over a period of two years--specifically the statistics associated with them.

The purpose of this analysis is to use linear regression and ensemble tree-based methods to predict the number of `shares`--our target variable--subset by data channel. For more details about these methods, see the _Modeling_ section.

According to the data description, there are 39,797 observations and 61 variables present. 58 of them are predictors, but using all of them would be inefficient and show glaring redundancies. Instead, the ones that will be used were chosen intuitively--what might encourage a user to share an article and what underlying forces may influence them:

1. `average_token_length` - Average length of the words in the content
2. `avg_negative_polarity` - Avg. polarity of negative words
3. `global_rate_positive_words` - Rate of positive words in the content
4. `global_sentiment_polarity` - Text sentiment polarity
5. `global_subjectivity` - Text subjectivity
6. `is_weekend` - Was the article published on the weekend?
7. `kw_avg_max` - Best keyword (avg. shares)
8. `kw_avg_min` - Worst Keyword (avg. shares)
9. `kw_max_max` - Best keyword (max shares)
10. `kw_min_min` - Worst keyword (min shares)
11. `n_non_stop_words` - Rate of non-stop words in the content
12. `n_tokens_content` - Number of words in the content
13. `n_tokens_title` - Number of words in the title
14. `n_unique_tokens` - Rate of unique words in the content
15. `num_hrefs` - Number of links
16. `num_imgs` - Number of images
17. `num_self_hrefs` - Number of links to other articles published by Mashable
18. `num_videos` - Number of videos
19. `self_reference_avg_shares` - Avg. shares of referenced articles in Mashable
20. `title_sentiment_polarity` - Title polarity

Which we store in a vector for fast retrieval:
```{r vars}
news.vars <- c('n_tokens_title', 'n_tokens_content', 'n_non_stop_words', 'num_hrefs', 'num_self_hrefs',
               'num_imgs', 'num_videos', 'average_token_length', 'kw_max_max', 'kw_min_min',
               'kw_avg_max', 'kw_avg_min', 'self_reference_avg_sharess','is_weekend', 'global_subjectivity',
               'global_sentiment_polarity', 'global_rate_positive_words', 'avg_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity')
```

## The Data

For the first iteration of our analysis, we will be using a single source for data channel. In other words, we will subset the data to one data channel here, and later, automate the process across all of the channels.

So, we read in the data and subset it to our chosen variables and the `Tech` data channel:

```{r read data}
news <- read_csv("OnlineNewsPopularity.csv")

tech.news <- news %>% filter(data_channel_is_tech == 1) %>% select(all_of(news.vars), shares) 
head(tech.news)
```

## Summarizations